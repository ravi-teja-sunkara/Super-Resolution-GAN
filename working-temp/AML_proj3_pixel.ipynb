{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add,Lambda\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.layers import add\n",
    "from keras.applications import VGG19\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def res_block_gen(gen, kernal_size, filters, strides):\n",
    "#     gen = model\n",
    "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(gen)\n",
    "    model = BatchNormalization(momentum = 0.5)(model)\n",
    "    # Using Parametric ReLU\n",
    "    model = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
    "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
    "    model = BatchNormalization(momentum = 0.5)(model)\n",
    "\n",
    "    model = Add()([gen, model])\n",
    "\n",
    "    return model\n",
    "\n",
    "# def SubpixelConv2D(input_shape, scale=2):\n",
    "#     \"\"\"\n",
    "#     Keras layer to do subpixel convolution.\n",
    "#     NOTE: Tensorflow backend only. Uses tf.depth_to_space\n",
    "#     Ref:\n",
    "#         [1] Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\n",
    "#             Shi et Al.\n",
    "#             https://arxiv.org/abs/1609.05158\n",
    "#     :param input_shape: tensor shape, (batch, height, width, channel)\n",
    "#     :param scale: upsampling scale. Default=4\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     # upsample using depth_to_space\n",
    "#     def subpixel_shape(input_shape):\n",
    "#         dims = [input_shape[0],\n",
    "#                 input_shape[1] * scale,\n",
    "#                 input_shape[2] * scale,\n",
    "#                 int(input_shape[3] / (scale ** 2))]\n",
    "#         output_shape = tuple(dims)\n",
    "#         return output_shape\n",
    "\n",
    "#     def subpixel(x):\n",
    "#         return tf.depth_to_space(x, scale)\n",
    "\n",
    "\n",
    "#     return Lambda(subpixel, output_shape=subpixel_shape, name='subpixel')\n",
    "\n",
    "# def content_loss(hr, sr):\n",
    "#     #vgg = srgan.vgg_54()\n",
    "#     vgg = VGG(image_HR_shape).vgg_19()\n",
    "#     sr = preprocess_input(sr)\n",
    "#     hr = preprocess_input(hr)\n",
    "#     sr_features = vgg(sr)\n",
    "#     hr_features = vgg(hr)\n",
    "#     return mean_squared_error(hr_features, sr_features)\n",
    "\n",
    "def SubpixelConv2D(scale):\n",
    "        return Lambda(lambda x: tf.depth_to_space(x, scale))\n",
    "\n",
    "\n",
    "def plotLoss(epoch,dLosses,gLosses):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(dLosses, label='Discriminitive loss')\n",
    "    plt.plot(gLosses, label='Generative loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('/home/vidyach/images/pixel/plots/dcgan_%d_loss_epoch.png' % epoch)\n",
    "    \n",
    "def up_sampling_block(model, kernal_size, filters, strides):\n",
    "   # model = UpSampling2D(size = 2)(model)\n",
    "    #model = Conv2DTranspose(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
    "    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n",
    "    \n",
    "    model = SubpixelConv2D(2)(model)\n",
    "    \n",
    "    model = LeakyReLU(alpha = 0.2)(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def discriminator_block(model, filters, kernel_size, strides):\n",
    "\n",
    "    model = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n",
    "    model = BatchNormalization(momentum = 0.5)(model)\n",
    "    model = LeakyReLU(alpha = 0.2)(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_gan_network(discriminator, shape, generator, optimizer, vgg):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=shape)\n",
    "    x = generator(gan_input)\n",
    "    x_feat = vgg(x)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=[x_feat,gan_output])\n",
    "    gan.compile(loss=[\"mse\", \"binary_crossentropy\"],\n",
    "                loss_weights=[1., 1e-3],\n",
    "                optimizer=optimizer)\n",
    "\n",
    "    return gan\n",
    "\n",
    "def normalize(input_data):\n",
    "\n",
    "    return (input_data.astype(np.float32) - 127.5)/127.5 \n",
    "\n",
    "def datagen(batchSize,filesList,filePath):\n",
    "    while(True):\n",
    "        files = np.random.choice(filesList,batchSize,replace=False)\n",
    "#         print(files)\n",
    "#         print(\"----------\")\n",
    "        X_train_HR = []\n",
    "        X_train_LR = []\n",
    "        for file in files:\n",
    "            image = cv2.imread(filePath + \"/\" + file)\n",
    "#             print(filePath + \"/\" + file)\n",
    "#             print(image)\n",
    "            image_HR = cv2.resize(image,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "            image_HR = normalize(image_HR)\n",
    "            #image_HR = image_HR / 255.0\n",
    "            X_train_HR.append(image_HR)\n",
    "            \n",
    "            image_LR = cv2.resize(image,(56,56),interpolation = cv2.INTER_CUBIC)\n",
    "            #image_LR = image_LR / 255.0\n",
    "            image_LR = normalize(image_LR)\n",
    "            \n",
    "            X_train_LR.append(image_LR)\n",
    "            \n",
    "        X_train_HR = np.array(X_train_HR)\n",
    "        X_train_LR = np.array(X_train_LR)\n",
    "        yield X_train_LR,X_train_HR\n",
    "        \n",
    "      \n",
    "    \n",
    "\n",
    "    \n",
    "def denormalize(input_data):\n",
    "    input_data = (input_data + 1) * 127.5\n",
    "    return input_data.astype(np.uint8)\n",
    "\n",
    "\n",
    "def plotGeneratedImages(output_dir,epoch,fake_img,low_img,hr_img,dim=(1, 3),figsize=(15, 5)):\n",
    "    \n",
    "    examples = hr_img.shape[0]\n",
    "    print(examples)\n",
    "    value = randint(0, examples)\n",
    "    image_batch_hr = denormalize(hr_img)\n",
    "    image_batch_lr = denormalize(low_img)\n",
    "    #gen_img = generator.predict(image_batch_lr)\n",
    "    #generated_image = denormalize(gen_img)\n",
    "    generated_image = denormalize(fake_img)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    plt.subplot(dim[0], dim[1], 1)\n",
    "    plt.imshow(image_batch_lr[value], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "        \n",
    "    plt.subplot(dim[0], dim[1], 2)\n",
    "    plt.imshow(generated_image[value], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(dim[0], dim[1], 3)\n",
    "    plt.imshow(image_batch_hr[value], interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir + 'generated_image_%d.png' % epoch)\n",
    "    \n",
    "    \n",
    "def saveModels(epoch,generator,discriminator):\n",
    "    fol = '/home/vidyach/models/pixel/'\n",
    "#     if not os.path.exists(fol):\n",
    "#         os.makedirs(fol)\n",
    "    generator.save(fol+'dcgan_generator_epoch_%d.h5' % epoch)\n",
    "    discriminator.save(fol+'dcgan_discriminator_epoch_%d.h5' % epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, noise_shape):\n",
    "        self.noise_shape = noise_shape\n",
    "        \n",
    "    def gen(self):\n",
    "        gen_Input = Input(shape = self.noise_shape)\n",
    "        \n",
    "        \n",
    "        model = Conv2D(filters = 64,kernel_size = 9 ,strides=1,padding = \"same\") (gen_Input)\n",
    "        model = temp = PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(model)\n",
    "        \n",
    "        for i in range(16):\n",
    "            model = res_block_gen(model,3,64,1)\n",
    "          \n",
    "        model = Conv2D(filters = 64,kernel_size=3,strides=1,padding=\"same\")(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        model = Add()([temp,model])\n",
    "            \n",
    "        for i in range(2):\n",
    "            model = up_sampling_block(model,3,256,1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        model = Conv2D(filters = 3,kernel_size=9,strides=1,padding=\"same\")(model)\n",
    "        model = Activation('tanh')(model)\n",
    "        \n",
    "        generator_model = Model(inputs = gen_Input, outputs = model)\n",
    "        \n",
    "        return generator_model\n",
    "    \n",
    "\n",
    "class Disciminator():\n",
    "    def __init__(self, image_shape):\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        \n",
    "    def disc(self):\n",
    "        disc_Input = Input(shape = self.image_shape)\n",
    "        \n",
    "        model = Conv2D(filters = 64,kernel_size = 3 ,strides=1,padding = \"same\") (disc_Input)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        \n",
    "        \n",
    "        model = discriminator_block(model, 64, 3, 2)\n",
    "        model = discriminator_block(model, 128, 3, 1)\n",
    "        model = discriminator_block(model, 128, 3, 2)\n",
    "        model = discriminator_block(model, 256, 3, 1)\n",
    "        model = discriminator_block(model, 256, 3, 2)\n",
    "        model = discriminator_block(model, 512, 3, 1)\n",
    "        model = discriminator_block(model, 512, 3, 2)\n",
    "        \n",
    "        model = Flatten()(model)\n",
    "        \n",
    "        model = Dense(1024)(model)\n",
    "        model = LeakyReLU(alpha = 0.2)(model)\n",
    "        \n",
    "        model = Dense(1)(model)\n",
    "        model = Activation('sigmoid')(model)\n",
    "        \n",
    "        disciminator_model = Model(inputs = disc_Input,output = model)\n",
    "        \n",
    "        \n",
    "        return disciminator_model\n",
    "    \n",
    "\n",
    "        \n",
    "class VGG(object):\n",
    "\n",
    "    def __init__(self, image_shape):\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    # computes VGG loss or content loss\n",
    "    def vgg_19(self):\n",
    "        \n",
    "        img = Input(shape = self.image_shape)\n",
    "        vgg = VGG19(weights=\"imagenet\")\n",
    "        vgg.outputs = [vgg.layers[9].output]\n",
    "        model = Model(inputs = img, outputs=vgg(img))\n",
    "        model.trainable = False\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the dataset\n",
    "# filePath_train_LR = 'Desktop/spring2019/Aml/proj3/dataset/DIV2K_train_LR_x8'\n",
    "# filePath_val_LR = 'Desktop/spring2019/Aml/proj3/dataset/DIV2K_valid_LR_x8'\n",
    "filePath_train_HR = '/home/vidyach/DIV2K_train_HR'\n",
    "#filePath_val_HR = 'Desktop/spring2019/Aml/proj3/dataset/DIV2K_valid_HR'\n",
    "#filePath_train_HR = 'Desktop/spring2019/Aml/proj3/dataset/DIV2K_train_HR'\n",
    "\n",
    "train_HR = [f for f in os.listdir(filePath_train_HR) ]\n",
    "#valid_HR = [f for f in os.listdir(filePath_val_HR) ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datagenObj = datagen(10,train_HR,filePath_train_HR)\n",
    "# temp = next(datagenObj)\n",
    "# print(temp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(temp[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_HR_shape = (224,224,3)\n",
    "image_LR_shape = (56,56,3)\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "# optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "no_of_epoch = 2000\n",
    "batch_count = 10\n",
    "d_loss = []\n",
    "g_loss = []\n",
    "filePathImage = \"/home/vidyach/images/pixel/plots/\"\n",
    "\n",
    "\n",
    "\n",
    "vgg = VGG(image_HR_shape).vgg_19()\n",
    "vgg.compile(loss = \"mse\", optimizer = optimizer)\n",
    "\n",
    "generator = Generator(image_LR_shape).gen()\n",
    "\n",
    "discriminator = Disciminator(image_HR_shape).disc()\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "\n",
    "gan = get_gan_network(discriminator, image_LR_shape, generator, optimizer, vgg)\n",
    "\n",
    "datagenObj = datagen(batch_count,train_HR,filePath_train_HR)\n",
    "\n",
    "for e in range(1,no_of_epoch):\n",
    "    for _ in tqdm(range(batch_count)):\n",
    "        lr_img,hr_img = next(datagenObj)\n",
    "        fake_img = generator.predict(lr_img)\n",
    "\n",
    "        real_data_Y = np.ones(batch_count) - np.random.random_sample(batch_count)*0.2\n",
    "        fake_data_Y = np.random.random_sample(batch_count)*0.2\n",
    "        dloss = 0\n",
    "        gan_loss = 0\n",
    "\n",
    "        if(e%2==0):\n",
    "        # training the disciminator\n",
    "            discriminator.trainable = True\n",
    "\n",
    "            d_loss_real = discriminator.train_on_batch(hr_img, real_data_Y)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_img, fake_data_Y)\n",
    "            dloss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "        else:\n",
    "            # training the combined network\n",
    "            hr_feat = vgg.predict(hr_img)\n",
    "            discriminator.trainable = False\n",
    "            gan_loss = gan.train_on_batch(lr_img, [hr_feat,real_data_Y])\n",
    "\n",
    "    d_loss.append(dloss)\n",
    "    g_loss.append(gan_loss)\n",
    "\n",
    "\n",
    "    if e == 1 or (e > 100 and e % 200 == 0):\n",
    "        saveModels(e,generator,discriminator)\n",
    "        plotGeneratedImages(filePathImage,e,fake_img,lr_img,hr_img)\n",
    "\n",
    "#print(\"--------------training complete-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()\n",
    "plotLoss(e,d_loss,g_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath_valid_HR = '/home/vidyach/DIV2K_valid_HR'\n",
    "# filePathImage_valid = \"/home/vidyach/images/plots/valid\"\n",
    "# valid_HR = [f for f in os.listdir(filePath_valid_HR)]\n",
    "\n",
    "# daagenObjValid = datagen(10,valid_HR,filePath_valid_HR)\n",
    "# epoch_size = 50\n",
    "\n",
    "# for e in range(1,epoch_size):\n",
    "#     lr_img,hr_img = next(daagenObjValid)\n",
    "#     gen_images = generator(lr_img)\n",
    "    \n",
    "    \n",
    "#     if e==1 or e%10==0:\n",
    "#         plotGeneratedImages(filePathImage_valid,e,gen_images,lr_img,hr_img)\n",
    "# print(\"----valid complete----\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
